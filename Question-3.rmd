gridExtra---
title: "Question 3 & 4"
output: html_document
---
```{r}
base_folder <- "/home/amirsalar/Diabetes-R/" # change this:)

data_folder <- paste0(base_folder, "data/")
output_folder <- paste0(base_folder, "output/")

diabetes_012_path <- paste0(data_folder,"diabetes_012_health_indicators_BRFSS2015.csv")
diabetes_binary_5050_path <- paste0(data_folder, "diabetes_binary_5050split_health_indicators_BRFSS2015.csv")
diabetes_binary_path <- paste0(data_folder, "diabetes_binary_health_indicators_BRFSS2015.csv")

library(data.table)
library(ggplot2)
library(readxl)
library(corrplot)
library(forcats)
library(gbm)
library(caret)
library(doParallel)
library(randomForest)
library(pROC)
library(verification)

dt_012 <- data.table(read.csv(diabetes_012_path))
dt_5050 <- data.table(read.csv(diabetes_binary_5050_path))
dt <- data.table(read.csv(diabetes_binary_path))

dt_5050 <- dt_5050[, Diabetes_binary := as.factor(Diabetes_binary)]
dt <- dt[, Diabetes_binary := as.factor(Diabetes_binary)]
```

# Question 3 & 4 answers

Yes we can. We can use different predictor subset selection methods and use cross validation in order to find best feature subset.
Because Random forests have fewer parameters to tune and are more popular compared to gbms libraries and functions are supported better, So will be continuing with them.

and for question 4 we will be evaluating our model using binary imbalance dataset and different metrics.

## Intro

We will use caret library in order to tune parameters and feature subset.
we use 10-fold cross validation in repeated mode.

## Model Selection

To select between different models we need a metric this can be :
1. AUC (so we can adjust the threshold and make our precision better)
2. F-Score (generally is a good model evaluator)
3. Precision
4. Accuracy
5. Sensitivity

## Feature Selection

now we apply last intuition to feature subset selection to select between different model with same number of features.(fix sized subsets)
and then we plot to see where we can optimize the trade-off between number of predictors and model accuracy!(We could've used metrics like AIC, BIC, Cp) and we use tolerance (1 percent) to choose less complex accurate models!

```{r}
set.seed(48)

cl <- makePSOCKcluster(9)
registerDoParallel(cl)

rfFuncs$summary <- prSummary
rfFuncs$selectSize <- function(x, metric, maximize) {
  pickSizeTolerance(x, metric, 1, maximize)
}

ctrl <- rfeControl(functions = rfFuncs,
                   method = "cv",
                   number = 10,
                   verbose = FALSE)

subsets <- c(1:11)
model.feature_selection <- rfe(
  dt_5050[, -"Diabetes_binary", with=FALSE],
  dt_5050[, Diabetes_binary],
  sizes = subsets,
  rfeControl = ctrl,
  metric = "F",
)

stopCluster(cl)
print(model.feature_selection)


predictors(model.feature_selection)
plot( model.feature_selection, type=c("g", "o"))
```

As we can see using range 5 predictors is optimal because after that the gain(F-score) compared to number of predictors is very little.
In the next block we will tune our model using these features.

Now Lets tune our model and find the best parameters for our random forest model inorder to do so we rewrite caret random forest model(because default caret only lets you grid search on mtry, and we want a full hyperparameter search)



```{r}
customRF <- list(label = "Random Forest",
                  library = "randomForest",
                  loop = NULL,
                  type = c("Classification", "Regression"),
                  parameters = data.frame(parameter = c("mtry", "ntree", "max_nodes"),
                                          class = c("numeric", "numeric", "numeric"),
                                          label = c("mtry", "ntree", "max_nodes")),
                  grid = function(x, y, len = NULL, search = "grid") {},
                  fit = function(x, y, wts, param, lev, last, classProbs, ...)
                    randomForest::randomForest(x, y, mtry = param$mtry, ntree=param$ntree, max_nodes=param$max_nodes,  ...),
                  predict = function(modelFit, newdata, submodels = NULL)
                    if(!is.null(newdata)) predict(modelFit, newdata) else predict(modelFit),
                  prob = function(modelFit, newdata, submodels = NULL)
                    if(!is.null(newdata)) predict(modelFit, newdata, type = "prob") else predict(modelFit, type = "prob"),
                  predictors = function(x, ...) {
                    ## After doing some testing, it looks like randomForest
                    ## will only try to split on plain main effects (instead
                    ## of interactions or terms like I(x^2).
                    varIndex <- as.numeric(names(table(x$forest$bestvar)))
                    varIndex <- varIndex[varIndex > 0]
                    varsUsed <- names(x$forest$ncat)[varIndex]
                    varsUsed
                  },
                  varImp = function(object, ...){
                    varImp <- randomForest::importance(object, ...)
                    if(object$type == "regression") {
                      if("%IncMSE" %in% colnames(varImp)) {
                        varImp <- data.frame(Overall = varImp[,"%IncMSE"])
                      } else {
                        varImp <- data.frame(Overall = varImp[,1])
                      }
                    }
                    else {
                      retainNames <- levels(object$y)
                      if(all(retainNames %in% colnames(varImp))) {
                        varImp <- varImp[, retainNames]
                      } else {
                        varImp <- data.frame(Overall = varImp[,1])
                      }
                    }

                    out <- as.data.frame(varImp, stringsAsFactors = TRUE)
                    if(dim(out)[2] == 2) {
                      tmp <- apply(out, 1, mean)
                      out[,1] <- out[,2] <- tmp
                    }
                    out
                  },
                  levels = function(x) x$classes,
                  tags = c("Random Forest", "Ensemble Model", "Bagging", "Implicit Feature Selection"),
                  sort = function(x) x[order(x[,1]),],
                  oob = function(x) {
                    out <- switch(x$type,
                                  regression =   c(sqrt(max(x$mse[length(x$mse)], 0)), x$rsq[length(x$rsq)]),
                                  classification =  c(1 - x$err.rate[x$ntree, "OOB"],
                                                      e1071::classAgreement(x$confusion[,-dim(x$confusion)[2]])[["kappa"]]))
                    names(out) <- if(x$type == "regression") c("RMSE", "Rsquared") else c("Accuracy", "Kappa")
                    out
                  })

```

Now lets grid search on parameters number of trees mtry(how many predictor to examine in splits) and max nodes.
We use metric F to choose between different models. We could've used precision or AUC or accuracy or etc.

```{r}
optVars <- c("GenHlth", "BMI","HighBP","Age","HighChol")
```

```{r}
cl <- makePSOCKcluster(3)
registerDoParallel(cl)



fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10,
  summaryFunction = prSummary,
  classProbs = TRUE,
)

dt_5050_optVars <- dt_5050[, c("Diabetes_binary", optVars), with=FALSE]
dt_optVars <- dt[, c("Diabetes_binary", optVars), with=FALSE]

random_indices <- sample(seq_len(nrow(dt_5050_optVars)), size = 45000, replace = FALSE)
smallerDT <- dt_5050_optVars[random_indices]

levels(smallerDT$Diabetes_binary) <- make.names(levels(smallerDT$Diabetes_binary), unique = TRUE)

random_forest.tune_grid <- expand.grid(mtry=1:3, ntree=(1:4) * 500, max_nodes=(1:5) * 5)


model.random_forest <- train(
Diabetes_binary ~ .,
  data = smallerDT,
  method = customRF,
  trControl = fitControl,
  metric = "F",
  tuneGrid = random_forest.tune_grid
)

stopCluster(cl)
print(model.random_forest)

```

now we can use any of the metrics above to choose the best parameters we can also change the summary, so we can use other metrics like accuracy(defaultSummary), recall(twoVariableSummary), etc.

### Split data to train and test

up until now we used  k-fold cross validation. but because data is imbalanced its better to test and evaluate our model using real world imbalanced data. and see it's metrics and performance.

```{r}



train <- list()
test <- list()

train$x_dt <- dt_5050_optVars[, -"Diabetes_binary", with=FALSE]
train$y_dt <- dt_5050_optVars[, Diabetes_binary]
test$x_dt <- dt_optVars[, -"Diabetes_binary", with=FALSE]
test$y_dt <-dt_optVars[, Diabetes_binary]


model.final <- randomForest::randomForest(
    x=train$x_dt,
    y=train$y_dt,
    formula = Diabetes_binary ~ .,
    x_test = test$x_dt,
    y_test = test$y_dt,
    mtry = model.random_forest$bestTune$mtry,
    ntree = model.random_forest$bestTune$ntree,
    max_nodes = model.random_forest$bestTune$max_nodes,
    norm.votes = TRUE
)

```

now lets print difference performance metrics for our model.

```{r}
classes <- c("0", "1")

preds <- as.factor(model.final$predicted)
levels(preds) <- classes

obs <- as.factor(model.final$y)
levels(obs) <- classes

model.random_forest.perf <- data.table(
  "obs" = obs,
  "pred" = preds,
  "1" = as.list(model.final$votes[, "1"]),
  "0" = as.list(model.final$votes[, "0"])
)

print(prSummary(model.random_forest.perf, lev=classes))
print(defaultSummary(model.random_forest.perf, lev=classes))
print(twoClassSummary(model.random_forest.perf, lev=classes))

```

We can see that our model has F-score of 73% and Precision of 76.5% , recall of 69% because we are sensitive to precision this is a good enough model to us, but we can use roc curve in order to make precision better (Precision is important because we want all diabetes to be detected even if we mis-classify no-diabetes
Lets see roc-curve.

```{r}
prediction <- predict(model.final, newdata=test$x_dt, type="prob")
roc.plot(as.integer(test$y_dt) - 1, 1 - prediction[,1])
```

