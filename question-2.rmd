---
title: "Question 2"
output: html_document
---
## Loading Data and preparing environment

### Importing libraries and setting paths
```{r}
base_folder <- "/home/amirsalar/Diabetes-R/" # change this:)

data_folder <- paste0(base_folder, "data/")
output_folder <- paste0(base_folder, "output/")

diabetes_012_path <- paste0(data_folder,"diabetes_012_health_indicators_BRFSS2015.csv")
diabetes_binary_5050_path <- paste0(data_folder, "diabetes_binary_5050split_health_indicators_BRFSS2015.csv")
diabetes_binary_path <- paste0(data_folder, "diabetes_binary_health_indicators_BRFSS2015.csv")

library(data.table)
library(ggplot2)
library(readxl)
library(corrplot)
library(forcats)
```
### Loading datasets
```{r}
dt_012 <- data.table(read.csv(diabetes_012_path))
dt_5050 <- data.table(read.csv(diabetes_binary_5050_path))
dt <- data.table(read.csv(diabetes_binary_path))
```

# Question 2 answer

### Different ways...
To find the most important features(predictors) we can use various methods like using F-statistics/p-value in logistic regression(our problem is classification) or see estimate how many branches are created in tree methods (random forest, boosting, naive tree...) because of that predictor and comparing different features using that.

## My choice

As I was looking that the data my mind went to Tree based methods, we have lots of categorical features and that just tree based methods playground a lot of categorical features. As for Logistic Regression on the contrary this is a huge disadvantage.


**checking number of different values in each predictor(to see how many of them are categorical)**
```{r}
sapply(colnames(dt_5050), function(x) length(unique(dt_5050[[x]])))
```
Having this many categorical features and specially binary features confirms my theory about using tree based methods.

## Using boosting

So lets use gradient boosting on decision trees and fit our data.
We don't tune parameters because we only need feature importance.


```{r}
library('gbm')

model.gbm <- gbm(
                Diabetes_binary ~.,
                data = dt_5050,
                distribution = "bernoulli",
                shrinkage = .01,
                n.minobsinnode = 10,
                n.trees = 500,
                interaction.depth = 2
            )


```

We choose the parameters based on insight and intuition in the next question we will see that we can use cross validation in order to choose best parameters.
I used
```{r}

model.feature_importance <- summary(model.gbm, plotit = FALSE)

ggplot(model.feature_importance, aes(rel.inf, forcats::fct_reorder(var, rel.inf))) +
  geom_col(fill="deepskyblue") +
  labs(title="Feature importance", x="Relative influence", y="Feature name")
```


We can see that we general Health, High Blood pressure, BMI, Age and High cholesterol are the main predictors for our prediction model,
they have the most influence (relative) meaning that if we calculate every predictors loss reduction separately on each tree and average it which predictor results in more loss reduction!

*(in question 3 we used random forest feature importance too)*
